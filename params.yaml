TrainingArguments:
  num_train_epochs: 5
  learning_rate: 0.00002
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  weight_decay: 0.01
  evaluation_strategy: "epoch"
  disable_tqdm: False
  logging_steps: 20
  log_level: "error"
  optim: "adamw_torch"